# -*- coding: utf-8 -*-
"""ProektISGP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1vqDUvFG6fmypQsQAW7YA9UlCenZ7LI

# Setting what we need for further action and data exploration
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy as sp
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, precision_score, \
recall_score, cohen_kappa_score, classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import StandardScaler
pd.options.display.max_columns = 999
pd.options.display.max_rows = 50

class color:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'

from google.colab import drive
drive.mount('/content/drive')

import csv
events = pd.read_csv(r'/content/drive/MyDrive/Proekt ETAI/events.csv')
info = pd.read_csv(r'/content/drive/MyDrive/Proekt ETAI/ginf.csv')
events = events.merge(info[['id_odsp', 'country', 'date']], on='id_odsp', how='left')
events

extract_year = lambda x: datetime.strptime(x, "%Y-%m-%d").year
events['year'] = [extract_year(x) for key, x in enumerate(events['date'])]

#Creating a new dataset that contains only the shot actions from the original dataset
shots = events[events.event_type==1]
shots['player'] = shots['player'].str.title()
shots['player2'] = shots['player2'].str.title()
shots['country'] = shots['country'].str.title()

sampled_shots, _ = train_test_split(shots, test_size=0.50, stratify=shots['is_goal'], random_state=42)

print(sampled_shots)

print("Original dataset distribution:")
print(shots['is_goal'].value_counts(normalize=True))
print("\nSampled dataset distribution:")
print(sampled_shots['is_goal'].value_counts(normalize=True))

pie = sampled_shots[['shot_outcome', 'id_event']].groupby('shot_outcome').count().reset_index().rename(columns={'id_event': 'count'})

pie = sampled_shots[['shot_outcome', 'id_event']].groupby('shot_outcome').count().reset_index().rename(columns={'id_event': 'count'})
pie['shot_outcome'] = pie['shot_outcome'].replace({1.0: 'On Target', 2.0: 'Off Target', 3.0: 'Blocked', 4.0: 'Hit the Bar'})

fig, ax = plt.subplots(figsize=[8,8])
labels = pie['shot_outcome']
colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']
plt.pie(x=pie['count'], autopct="%.1f%%", labels=labels, explode=[0.06]*4, pctdistance=0.7, colors=colors, shadow=True, \
       textprops=dict(fontsize=16))
plt.title("Shot Outcomes", fontsize=26, fontfamily='serif')
plt.tight_layout()
plt.show()

bar = sampled_shots[['shot_place', 'id_event']].groupby('shot_place').count().reset_index().rename(columns={'id_event': 'count'})

# Map 'shot_place' to its descriptive labels
shot_place_mapping = {
    1: 'Bit too high',
    2: 'Blocked',
    3: 'Bottom left corner',
    4: 'Bottom right corner',
    5: 'Centre of the goal',
    6: 'High and wide',
    7: 'Hits the bar',
    8: 'Misses to the left',
    9: 'Misses to the right',
    10: 'Too high',
    11: 'Top centre of the goal',
    12: 'Top left corner',
    13: 'Top right corner'
}
bar['shot_place'] = bar['shot_place'].map(shot_place_mapping)

sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=[13, 6])
colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
ax = sns.barplot(x='count', y='shot_place', data=bar, palette='muted')
ax.set_yticklabels(bar['shot_place'], size=13)
ax.set_xticks(np.arange(1000, 30000, 4000))
ax.set_ylabel('')
plt.title("Shot Placement", fontsize=25, fontfamily='serif')
plt.tight_layout()
ax.grid(color='black', linestyle='-', linewidth=0.1, axis='x')
plt.show()

data = pd.get_dummies(sampled_shots.iloc[:,-8:-3], columns=['location', 'bodypart','assist_method', 'situation'])
data.columns = ['fast_break', 'loc_centre_box', 'loc_diff_angle_lr', 'diff_angle_left', 'diff_angle_right',
                'left_side_box', 'left_side_6ybox', 'right_side_box', 'right_side_6ybox', 'close_range',
                'penalty', 'outside_box', 'long_range', 'more_35y', 'more_40y', 'not_recorded', 'right_foot',
                'left_foot', 'header', 'no_assist', 'assist_pass', 'assist_cross', 'assist_header',
                'assist_through_ball', 'open_play', 'set_piece', 'corner', 'free_kick']
data['is_goal'] = sampled_shots['is_goal']

print(len(data))
print(data.is_goal.sum())
print(len(data.columns)-1)

"""# Training/creating the Gradient Boost model"""

X = data.iloc[:,:-1]
y = data.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=1)

from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

def evaluate_model(params):
    model = GradientBoostingClassifier(
                        learning_rate=params['learning_rate'],
                        min_samples_leaf=params['min_samples_leaf'],
                        max_depth = params['max_depth'],
                        max_features = params['max_features']
                        )

    model.fit(X_train, y_train)
    return {
        'learning_rate': params['learning_rate'],
        'min_samples_leaf': params['min_samples_leaf'],
        'max_depth': params['max_depth'],
        'max_features': params['max_features'],
        'train_ROCAUC': roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]),
        'test_ROCAUC': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]),
        'recall': recall_score(y_test, model.predict(X_test)),
        'precision': precision_score(y_test, model.predict(X_test)),
        'f1_score': f1_score(y_test, model.predict(X_test)),
        'train_accuracy': model.score(X_train, y_train),
        'test_accuracy': model.score(X_test, y_test),
    }

def objective(params):
    res = evaluate_model(params)

    res['loss'] = - res['test_ROCAUC']
    res['status'] = STATUS_OK
    return res

hyperparameter_space = {
        'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),
        'min_samples_leaf': hp.choice('min_samples_leaf', range(15, 200)),
        'max_depth': hp.choice('max_depth', range(2, 20)),
        'max_features': hp.choice('max_features', range(3, 27))
}

trials = Trials()
fmin(
    objective,
    space=hyperparameter_space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials
);

pd.DataFrame(trials.results).sort_values(by='f1_score', ascending=False).head(5)

model = GradientBoostingClassifier(
                        learning_rate=0.287721,
                        min_samples_leaf=106,
                        max_depth = 16,
                        max_features = 17
                        )
model.fit(X_train, y_train)

importances = model.feature_importances_
feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f'Feature {i}' for i in range(X_train.shape[1])]
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print('\nFeature Importances:')
print(feature_importance_df)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Gradient boosting Feature Importances')
plt.gca().invert_yaxis()
plt.show()

print('The test set contains {} examples (shots) of which {} are positive (goals).'.format(len(y_test), y_test.sum()))
print('The accuracy of classifying whether a shot is goal or not is {}%.'.format(round(model.score(X_test, y_test)*100),2))
print('Our classifier obtains an ROC-AUC of {}%'.format(round(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])*100),2))

print('The baseline performance for PR-AUC is {}%. This is the PR-AUC that what we would get by random guessing.'.format(round(y_train.mean(),2)))
print('Our model obtains an PR-AUC of {}%.'.format(round(average_precision_score(y_test, model.predict_proba(X_test)[:, 1])*100,2)))
print('Our classifier obtains a Cohen Kappa of {}.'.format(round(cohen_kappa_score(y_test,model.predict(X_test)),2)))

print(color.BOLD + color.YELLOW + 'Confusion Matrix:\n' + color.END)
print(confusion_matrix(y_test,model.predict(X_test)))
print(color.BOLD +  color.YELLOW + '\n Report:' + color.END)
print(classification_report(y_test,model.predict(X_test)))

"""# Gradient Boosting vs Random forest vs Logistic Regression  """

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report
import numpy as np
import pandas as pd

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# I am defining a smaller parameter grid for RandomizedSearchCV for time sake
param_distributions = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt'],
    'max_depth': [10, 50, 100, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True, False]
}

# Creating the base model
rf = RandomForestRegressor()

# Performing the randomized search with cross-validation
rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=10,
                               cv=3, verbose=2, random_state=42, n_jobs=-1)

rf_random.fit(X_train, y_train)

print('Best Parameters:', rf_random.best_params_)

# Use the best estimator to predict on the test set
best_rf = rf_random.best_estimator_
y_pred = best_rf.predict(X_test)

#Just checking the MAE and R^2 even though I do not use them for comparison with the other models
print('The test set contains {} examples (shots).'.format(len(y_test)))
print('Mean Squared Error: {:.2f}'.format(mean_squared_error(y_test, y_pred)))
print('R^2 Score: {:.2f}'.format(r2_score(y_test, y_pred)))

# For classification metrics, we need to threshold the probabilities
threshold = 0.5  # You can adjust this threshold
y_pred_class = (y_pred >= threshold).astype(int)
y_test_class = (y_test >= threshold).astype(int)

print('Accuracy of classifying whether a shot is a goal or not is {:.2f}%.'.format(np.mean(y_pred_class == y_test_class) * 100))
print('Our classifier obtains an ROC-AUC of {:.2f}%.'.format(roc_auc_score(y_test_class, y_pred) * 100))
print('The baseline performance for PR-AUC is {:.2f}%. This is the PR-AUC that we would get by random guessing.'.format(y_train.mean() * 100))
print('Our model obtains a PR-AUC of {:.2f}%.'.format(average_precision_score(y_test_class, y_pred) * 100))

print('Confusion Matrix:\n')
print(confusion_matrix(y_test_class, y_pred_class))
print('\nClassification Report:\n')
print(classification_report(y_test_class, y_pred_class))

# Feature importance
importances = best_rf.feature_importances_
feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f'Feature {i}' for i in range(X_train.shape[1])]
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print('\nFeature Importances:')
print(feature_importance_df)

# Plot the feature importances
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Random forest Feature Importances')
plt.gca().invert_yaxis()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Again I am defining a smaller parameter grid for time sake
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': np.logspace(-4, 4, 10),
    'solver': ['lbfgs', 'liblinear', 'saga'],
    'max_iter': [100]
}

lr = LogisticRegression()

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=lr, param_grid=param_grid,
                           scoring='roc_auc', cv=3, verbose=2, n_jobs=-1)

grid_search.fit(X_train, y_train)

# Display the best parameters
print('Best Parameters:', grid_search.best_params_)

from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Fit the Logistic Regression model with the best parameters
best_params = grid_search.best_params_.copy()
if 'max_iter' in best_params:
    del best_params['max_iter']

model = LogisticRegression(max_iter=400, **best_params)
model.fit(X_train, y_train)

# Get feature importances (coefficients) because it's a logistic regression model
coefficients = model.coef_[0]
feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f'Feature {i}' for i in range(X_train.shape[1])]
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': np.abs(coefficients)
}).sort_values(by='Importance', ascending=False)

print('\nFeature Importances for Logistic Regression:')
print(feature_importance_df)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Absolute Coefficient Value')
plt.title('Feature Importances for Logistic Regression')
plt.gca().invert_yaxis()
plt.show()

print('The test set contains {} examples (shots) of which {} are positive (goals).'.format(len(y_test), y_test.sum()))
print('The accuracy of classifying whether a shot is goal or not is {}%.'.format(round(model.score(X_test, y_test)*100),2))
print('Our classifier obtains an ROC-AUC of {}%'.format(round(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])*100),2))

print('The baseline performance for PR-AUC is {}%. This is the PR-AUC that what we would get by random guessing.'.format(round(y_train.mean(),2)))
print('Our model obtains an PR-AUC of {}%.'.format(round(average_precision_score(y_test, model.predict_proba(X_test)[:, 1])*100,2)))
print('Our classifier obtains a Cohen Kappa of {}.'.format(round(cohen_kappa_score(y_test,model.predict(X_test)),2)))

print(color.BOLD + color.YELLOW + color.UNDERLINE + 'Confusion Matrix:\n' + color.END)
print(confusion_matrix(y_test,model.predict(X_test)))
print(color.BOLD +  color.YELLOW + color.UNDERLINE + '\n Report:' + color.END)
print(classification_report(y_test,model.predict(X_test)))

"""# PLAYER ANALYSIS:

"""

X_sampled = X.loc[sampled_shots.index]
sampled_shots['prediction'] = model.predict_proba(X_sampled)[:, 1]
sampled_shots['difference'] = sampled_shots['prediction'] - sampled_shots['is_goal']
print(sampled_shots.head())

players = sampled_shots.groupby('player').sum().reset_index()
players.rename(columns={'is_goal': 'trueGoals', 'prediction': 'expectedGoals'}, inplace=True)
players.expectedGoals = round(players.expectedGoals,2)
players.difference = round(players.difference,2)
players['ratio'] = players['trueGoals'] / players['expectedGoals']

print(round(players.expectedGoals.corr(players.trueGoals),3))

"""The correlation between true and expected goals is high which shows that our model is good."""

#Players with the highest difference between true and expected goals
show = players.sort_values(['difference', 'trueGoals']).reset_index(drop=True)
show['rank'] = show.index+1
show = show[['rank', 'player', 'difference', 'trueGoals', 'expectedGoals']].head(10)
show.head(5)

"""Players with most accumulated Xg."""

show = players[['player', 'trueGoals', 'expectedGoals']].sort_values(['expectedGoals'], ascending=False).head(10)
show.head(5)

"""Players that take bad shots (Shots where we have low value for XG), I am taking players with minimum 50 taken shots"""

players.rename(columns={'event_type': 'n_shots'}, inplace=True)

players['xG_per_shot_ratio'] = players['expectedGoals'] / players['n_shots']
show = players[players['n_shots']>50].sort_values(['xG_per_shot_ratio', 'trueGoals'], ascending=False).reset_index(drop=True)
show['rank'] = show.index+1
show[['rank', 'player', 'xG_per_shot_ratio', 'trueGoals', 'expectedGoals', 'difference']].tail(5)

"""Players whose shots accumulated the most XG/per shot"""

show = show[['rank', 'player', 'xG_per_shot_ratio', 'trueGoals', 'expectedGoals', 'difference']].head(10)
show.head(5)

sns.set_style("dark")
fig, ax = plt.subplots(figsize=[12,5])
ax = sns.barplot(x=show['xG_per_shot_ratio'], y=show['player'], palette='viridis', alpha=0.9)
ax.set_xticks(np.arange(0,0.25,0.02))
ax.set_xlabel(xlabel='xG value per shot', fontsize=12)
ax.set_ylabel(ylabel='')
ax.set_yticklabels(labels=ax.get_yticklabels(), fontsize=12)
plt.title("Best Shot Deciders: xG value per shot", fontsize=22, fontfamily='serif')
ax.grid(color='black', linestyle='-', linewidth=0.1, axis='x')
plt.show()

"""Best headers of the ball"""

headers = sampled_shots[(sampled_shots.event_type==1) & (sampled_shots.bodypart==3)]
headers_players = headers.groupby('player').sum().reset_index()
headers_players.rename(columns={'event_type': 'n_headers', 'is_goal': 'trueGoals', 'prediction': 'expectedGoals'}, inplace=True)
headers_players['ratio'] = headers_players['trueGoals'] / headers_players['expectedGoals']
show_headers = headers_players.sort_values(['difference', 'trueGoals']).reset_index(drop=True)
show_headers['rank'] = show_headers.index+1
show_headers[['rank', 'player', 'n_headers', 'trueGoals', 'expectedGoals', 'difference']].head(5)

"""Best left footed players

"""

left_foot = sampled_shots[(sampled_shots.event_type==1) & (sampled_shots.bodypart==2)]
left_foot_players = left_foot.groupby('player').sum().reset_index()
left_foot_players.rename(columns={'event_type': 'n_leftFoot_shots', 'is_goal': 'trueGoals', 'prediction': 'expectedGoals'}, inplace=True)
show_lfoot = left_foot_players.sort_values(['difference', 'trueGoals']).reset_index(drop=True)
show_lfoot['rank'] = show_lfoot.index+1
show_lfoot[['rank', 'player', 'n_leftFoot_shots', 'trueGoals', 'expectedGoals', 'difference']].head(5)

#Just to show that we can check every single metric that we like for every single individual player
show_lfoot[show_lfoot.player=='Eden Hazard'][['rank', 'player', 'expectedGoals', 'trueGoals']]

"""LEFT FOOT SHOT TO GOALS RATIO:"""

left_foot_players['ratio'] = left_foot_players['trueGoals'] / left_foot_players['expectedGoals']
show_lfoot = left_foot_players[left_foot_players['trueGoals']>20].sort_values(['ratio', 'trueGoals'], ascending=False).reset_index(drop=True)
show_lfoot['rank'] = show_lfoot.index+1
show_lfoot[['rank', 'player', 'n_leftFoot_shots', 'trueGoals', 'expectedGoals', 'ratio']].head(5)

"""BEST RIGHT FOOTERS:"""

right_foot = sampled_shots[(sampled_shots.event_type==1) & (sampled_shots.bodypart==1)]
right_foot_players = right_foot.groupby('player').sum().reset_index()
right_foot_players.rename(columns={'event_type': 'n_rightFoot_shots', 'is_goal': 'trueGoals', 'prediction': 'expectedGoals'}, inplace=True)
show = right_foot_players.sort_values(['difference', 'trueGoals']).reset_index(drop=True)
show['rank'] = show.index+1
show[['rank', 'player', 'n_rightFoot_shots', 'trueGoals', 'expectedGoals', 'difference']].head(5)

#Right foot true to expected goals ratio
right_foot_players['ratio'] = right_foot_players['trueGoals'] / right_foot_players['expectedGoals']
show_rfoot = right_foot_players[right_foot_players['trueGoals']>20].sort_values(['ratio', 'trueGoals'], ascending=False).reset_index(drop=True)
show_rfoot['rank'] = show_rfoot.index+1
show_rfoot[['rank', 'player', 'n_rightFoot_shots', 'trueGoals', 'expectedGoals', 'ratio']].head(5)

"""BEST OUTSIDE THE BOX SHOOTERS"""

outside_box = sampled_shots[(sampled_shots.location==15)]
outbox_players = outside_box.groupby('player').sum().reset_index()
outbox_players.rename(columns={'event_type': 'n_outbox_shots', 'is_goal': 'trueGoals', 'prediction': 'expectedGoals'}, inplace=True)
show = outbox_players.sort_values(['difference', 'trueGoals']).reset_index(drop=True)
show['rank'] = show.index+1
show[['rank', 'player', 'n_outbox_shots', 'trueGoals', 'expectedGoals', 'difference']].head(5)

show[['rank', 'player', 'n_outbox_shots', 'trueGoals', 'expectedGoals', 'difference']].tail(5)

#True-Expected goals ratio
outbox_players['ratio'] = outbox_players['trueGoals'] / outbox_players['expectedGoals']
show = outbox_players[outbox_players['n_outbox_shots']>80].sort_values(['ratio', 'trueGoals'], ascending=False).reset_index(drop=True)
show['rank'] = show.index+1
show = show[['rank', 'player', 'n_outbox_shots', 'trueGoals', 'expectedGoals', 'ratio']].head(10)
show.head(5)

sns.set_style("dark")
fig, ax = plt.subplots(figsize=[14,6])
ax = sns.barplot(x=show['ratio'], y=show['player'], palette='viridis', alpha=0.9)
ax.set_xticks(np.arange(0,4,0.3))
ax.set_xlabel(xlabel='Long Range Goals per XGoal Ratio', fontsize=14)
ax.set_ylabel(ylabel='')
ax.set_yticklabels(labels=ax.get_yticklabels(), fontsize=12)
plt.title("Best Long Range Shooters", fontsize=22, fontfamily='serif')
ax.grid(color='black', linestyle='-', linewidth=0.1, axis='x')
plt.show()

"""BEST PLAYMAKERS (MAKING PASSES THAT RESULT IN POSITIONS FROM WHICH A LARGE AMMOUNT OF XG IS ACCUMULATED)"""

passing = sampled_shots[sampled_shots.assist_method.isin([1,4])]
passing_players = passing.groupby('player2').sum().reset_index()
passing_players.rename(columns={'player2': 'passer', 'event_type': 'n_passes', 'is_goal': 'trueGoals_created', 'prediction': 'expectedGoals_created'}, inplace=True)
show = passing_players.sort_values('expectedGoals_created', ascending=False).reset_index(drop=True)
show['rank'] = show.index+1
show[['rank', 'passer', 'n_passes', 'trueGoals_created', 'expectedGoals_created']].head(5)

"""XG PER PASS"""

show['xG_perpass'] = show['expectedGoals_created'] / show['n_passes']
show = show[show['n_passes']>80].sort_values('xG_perpass', ascending=False).reset_index(drop=True)
show['rank'] = show.index+1
show = show[['rank', 'passer', 'n_passes', 'xG_perpass']].head(10)
show.head(5)

"""MOST UNLUCKY PASSERS"""

passing = sampled_shots[sampled_shots.assist_method.isin([1,4])]
passing_players = passing.groupby('player2').sum().reset_index()
passing_players.rename(columns={'player2': 'passer', 'event_type': 'n_passes', 'is_goal': 'trueGoals_created', 'prediction': 'expectedGoals_created'}, inplace=True)
show = passing_players[passing_players['n_passes']>50].sort_values('difference', ascending=False).reset_index(drop=True)
show['rank'] = show.index+1
show[['rank', 'passer', 'n_passes', 'trueGoals_created', 'expectedGoals_created', 'difference']].head(5)